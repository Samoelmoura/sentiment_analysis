{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b621b84",
   "metadata": {},
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdbc098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:20:38.108294Z",
     "start_time": "2022-04-10T23:20:35.030975Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f4b62",
   "metadata": {},
   "source": [
    "## 0.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032d899a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:20:39.045100Z",
     "start_time": "2022-04-10T23:20:38.108294Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2057279074</td>\n",
       "      <td>Sat Jun 06 12:42:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PolloLoco66</td>\n",
       "      <td>Ukraine 2 Croatia 2 after 68 minutes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2204150887</td>\n",
       "      <td>Wed Jun 17 01:27:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kaylaSTACK</td>\n",
       "      <td>@courtney_xxx out of shower , picked up phone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2047790284</td>\n",
       "      <td>Fri Jun 05 14:11:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>smacula</td>\n",
       "      <td>@S4BI i am a tweeting god... hahahah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1972065032</td>\n",
       "      <td>Sat May 30 08:29:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lorena182</td>\n",
       "      <td>@charmingsharo 17 again ..... again ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1823113262</td>\n",
       "      <td>Sat May 16 21:08:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>leleana</td>\n",
       "      <td>@sarahactually OMG, your cats name is Pippin? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query         user  \\\n",
       "0          0  2057279074  Sat Jun 06 12:42:20 PDT 2009  NO_QUERY  PolloLoco66   \n",
       "1          0  2204150887  Wed Jun 17 01:27:26 PDT 2009  NO_QUERY   kaylaSTACK   \n",
       "2          4  2047790284  Fri Jun 05 14:11:06 PDT 2009  NO_QUERY      smacula   \n",
       "3          4  1972065032  Sat May 30 08:29:19 PDT 2009  NO_QUERY    Lorena182   \n",
       "4          0  1823113262  Sat May 16 21:08:49 PDT 2009  NO_QUERY      leleana   \n",
       "\n",
       "                                                text  \n",
       "0            Ukraine 2 Croatia 2 after 68 minutes.    \n",
       "1  @courtney_xxx out of shower , picked up phone,...  \n",
       "2              @S4BI i am a tweeting god... hahahah   \n",
       "3            @charmingsharo 17 again ..... again ;)   \n",
       "4  @sarahactually OMG, your cats name is Pippin? ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:\\\\My Drive\\\\Pessoal\\\\Projetos\\\\sentiment_analysis\\\\training.1600000.processed.noemoticon.csv', header=None, encoding='latin1')\n",
    "\n",
    "df.columns= 'sentiment id date query user text'.split()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27c887",
   "metadata": {},
   "source": [
    "## 0.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406ee618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:20:39.060904Z",
     "start_time": "2022-04-10T23:20:39.047034Z"
    }
   },
   "outputs": [],
   "source": [
    "# data clean\n",
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+', ' ', tweet)\n",
    "    tweet = re.sub(r'https?://[A-Za-z0-9./]+', ' ', tweet)\n",
    "    tweet = re.sub(r\"[^A-Za-z.!?']\", ' ', tweet)\n",
    "    tweet = re.sub(r' +', ' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "# encoding \n",
    "def encode_sentence(sent):\n",
    "    return token_model.convert_tokens_to_ids(token_model.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29bc594",
   "metadata": {},
   "source": [
    "# 1.0 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2134a",
   "metadata": {},
   "source": [
    "## 1.1 Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22876522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:20:39.075741Z",
     "start_time": "2022-04-10T23:20:39.062745Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H치 no total: 400000 linhas\n",
      "H치 no total: 6 columnas\n"
     ]
    }
   ],
   "source": [
    "print(f'H치 no total: {df.shape[0]} linhas')\n",
    "print(f'H치 no total: {df.shape[1]} columnas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec64b83",
   "metadata": {},
   "source": [
    "## 1.2 Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0b4e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:20:39.091712Z",
     "start_time": "2022-04-10T23:20:39.078808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment     int64\n",
       "id            int64\n",
       "date         object\n",
       "query        object\n",
       "user         object\n",
       "text         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0746c4",
   "metadata": {},
   "source": [
    "## 1.3 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82879a36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:20:39.250474Z",
     "start_time": "2022-04-10T23:20:39.093718Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "id           0\n",
       "date         0\n",
       "query        0\n",
       "user         0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942860e4",
   "metadata": {},
   "source": [
    "# 2.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce439ba",
   "metadata": {},
   "source": [
    "###### 2.1.1 Changing positive sentiments values to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faca0ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:20:39.265539Z",
     "start_time": "2022-04-10T23:20:39.252475Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['sentiment']==4, 'sentiment'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b328a",
   "metadata": {},
   "source": [
    "###### 2.1.2 Removing Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7413a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:22:12.625034Z",
     "start_time": "2022-04-10T23:20:39.266476Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c402edf",
   "metadata": {},
   "source": [
    "# 3.0 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c313af5",
   "metadata": {},
   "source": [
    "## 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f97cecf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:22:21.271650Z",
     "start_time": "2022-04-10T23:22:12.625034Z"
    }
   },
   "outputs": [],
   "source": [
    "# trained algotithm\n",
    "bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4', trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bcd30aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:22:21.288960Z",
     "start_time": "2022-04-10T23:22:21.271650Z"
    }
   },
   "outputs": [],
   "source": [
    "# vocab dataframe file\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02a08e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:22:21.344871Z",
     "start_time": "2022-04-10T23:22:21.288960Z"
    }
   },
   "outputs": [],
   "source": [
    "# token definition\n",
    "token_model = BertTokenizer(vocab_file, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b9511a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:24:01.961038Z",
     "start_time": "2022-04-10T23:22:21.347213Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoding\n",
    "df['text'] = df['text'].apply(encode_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347276ad",
   "metadata": {},
   "source": [
    "## 3.3 Data Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16510261",
   "metadata": {},
   "source": [
    "###### 5.1 Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c318ce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:24:02.061229Z",
     "start_time": "2022-04-10T23:24:01.961038Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['id', 'date', 'query', 'user'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefd822",
   "metadata": {},
   "source": [
    "## 3.4 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6a695b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:24:03.414905Z",
     "start_time": "2022-04-10T23:24:02.068828Z"
    }
   },
   "outputs": [],
   "source": [
    "# changing dataframe to lists\n",
    "sentence = df['text'].to_list()\n",
    "sentiment = df['sentiment'].to_list()\n",
    "\n",
    "dataset = [[sent, sentiment[i], len(sent)] for i, sent in enumerate(sentence)]\n",
    "random.shuffle(dataset)\n",
    "dataset = [(sent[0], sent[1])\n",
    "          for sent in dataset if sent[2] > 7] # droping rows with 7 words or less\n",
    "\n",
    "# parameters\n",
    "BATCH_SIZE = 32\n",
    "NB_BATCHES = len(df) // BATCH_SIZE\n",
    "NB_BATCHES_TEST = NB_BATCHES // 10\n",
    "\n",
    "# changing dataframe to tensorflow format\n",
    "tf_dataset = tf.data.Dataset.from_generator(lambda: dataset, output_types=(tf.int32, tf.int32))\n",
    "\n",
    "# adding padds\n",
    "all_batched = tf_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None,), ()))\n",
    "\n",
    "# shuffle\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "\n",
    "# splits\n",
    "test = all_batched.take(NB_BATCHES_TEST)\n",
    "train = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f46fae",
   "metadata": {},
   "source": [
    "# 4 Embeging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c4fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32769730",
   "metadata": {},
   "source": [
    "# 6.0 Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a45853",
   "metadata": {},
   "source": [
    "## 6.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2dc32c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:24:03.462995Z",
     "start_time": "2022-04-10T23:24:03.414905Z"
    }
   },
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 emb_dim = 128,\n",
    "                 nb_filters = 50, \n",
    "                 FFN_units = 512,\n",
    "                 nb_classes = 2,\n",
    "                 dropout_rate = 0.1,\n",
    "                 training = False,\n",
    "                 name = 'dcnn'\n",
    "                ):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "\n",
    "        self.embedding = layers.Embedding(VOCAB_SIZE, EMB_DIM)\n",
    "\n",
    "        self.bigram = layers.Conv1D(filters = NB_FILTERS,\n",
    "                                   kernel_size = 2,\n",
    "                                   padding = 'valid',\n",
    "                                   activation = 'relu')\n",
    "\n",
    "        self.trigram = layers.Conv1D(filters = NB_FILTERS,\n",
    "                                   kernel_size = 3,\n",
    "                                   padding = 'valid',\n",
    "                                   activation = 'relu')\n",
    "\n",
    "        self.fourgram = layers.Conv1D(filters = NB_FILTERS,\n",
    "                                   kernel_size = 4,\n",
    "                                   padding = 'valid',\n",
    "                                   activation = 'relu')\n",
    "\n",
    "        self.pool = layers.GlobalAvgPool1D()\n",
    "\n",
    "        self.dense_1 = layers.Dense(units = FFN_UNITS, activation = 'relu')\n",
    "\n",
    "        self.dropout = layers.Dropout(rate = DROPOUT_RATE)\n",
    "\n",
    "        if NB_CLASSES == 2:\n",
    "            self.last_dense = layers.Dense(units = 1, activation = 'sigmoid')\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units = NB_CLASSES, activation = 'softmax')\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "\n",
    "        return output\n",
    "\n",
    "# parameters\n",
    "EMB_DIM = 200\n",
    "VOCAB_SIZE = len(token_model)\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "# instanciate model\n",
    "Dcnn = DCNN(vocab_size = VOCAB_SIZE,\n",
    "           emb_dim = EMB_DIM, \n",
    "           nb_filters = NB_FILTERS,\n",
    "           FFN_units = FFN_UNITS,\n",
    "           nb_classes = NB_CLASSES,\n",
    "           dropout_rate = DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2df972",
   "metadata": {},
   "source": [
    "## 6.2 Creating a Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d69f26e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T23:38:48.712561Z",
     "start_time": "2022-04-10T23:38:48.676079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!\n"
     ]
    }
   ],
   "source": [
    "if NB_CLASSES ==2:\n",
    "    Dcnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "else:\n",
    "    Dcnn.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "checkpoint_patch = 'D:\\\\My Drive\\\\Pessoal\\\\Projetos\\\\sentiment_analysis\\\\exports'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_patch, max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!')\n",
    "\n",
    "class MyCustomCallBack(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(f'Checkpoint saved at: {checkpoint_patch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f308b8",
   "metadata": {},
   "source": [
    "## 6.3 Fiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b2a81",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-10T23:38:48.675Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   5368/Unknown - 458s 85ms/step - loss: 0.3483 - accuracy: 0.8481"
     ]
    }
   ],
   "source": [
    "history = Dcnn.fit(train,\n",
    "                  epochs=NB_EPOCHS,\n",
    "                  callbacks=[MyCustomCallBack()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
